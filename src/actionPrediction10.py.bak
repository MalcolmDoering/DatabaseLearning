'''
Created on Nov 15, 2018

@author: MalcolmD


modified from actionPrediction8

explicitly match input to camera and attribute before extracting value from database
'''



import tensorflow as tf

import numpy as np
from six.moves import range
from datetime import datetime
from sklearn import metrics
import editdistance
import csv


import tools
from copynet import copynet


print "tensorflow version", tf.__version__


sessionDir = tools.create_session_dir("actionPrediction8_dbl")

eosChar = "#"
goChar = "~"

tf.reset_default_graph()
tf.set_random_seed(0)


class CustomNeuralNetwork(object):
    
    def __init__(self, inputSeqLen, dbSeqLen, outputSeqLen, batchSize, numUniqueCams, numUniqueAtts, vocabSize, embeddingSize, charToIndex):
        
        self.inputSeqLen = inputSeqLen
        self.dbSeqLen = dbSeqLen
        self.outputSeqLen = outputSeqLen
        self.batchSize = batchSize
        self.numUniqueCams = numUniqueCams
        self.numUniqueAtts = numUniqueAtts
        self.vocabSize = vocabSize
        self.embeddingSize = embeddingSize
        self.charToIndex = charToIndex
        
        self.gumbel_softmax_temp = 2
        
        #
        # build the model
        #
        
        #with tf.name_scope("input encoder"):
        self._inputs = tf.placeholder(tf.int32, [self.batchSize, self.inputSeqLen, ], name='customer_inputs')
        self._input_one_hot = tf.one_hot(self._inputs, self.vocabSize)
            
        
        with tf.variable_scope("input_encoder_1"):
            # input encoder for the initial state of the copynet
            
            num_units = [self.embeddingSize, self.embeddingSize]
            #cells = [tf.nn.rnn_cell.LSTMCell(num_units=n, initializer=tf.initializers.glorot_normal()) for n in num_units]
            cells = [tf.nn.rnn_cell.GRUCell(num_units=n, kernel_initializer=tf.initializers.glorot_normal()) for n in num_units]
            
            stacked_rnn_cell = tf.contrib.rnn.MultiRNNCell(cells)
            
            _, input_encoding = tf.nn.dynamic_rnn(stacked_rnn_cell, self._input_one_hot, dtype=tf.float32)
            
            # for single layer GRU
            self._input_encoding = input_encoding[-1] # TODO why is this here??? [-1] A: get output instead of candidate 
        
            # for two layer LSTM
            #self._input_encoding = tf.concat([input_encoding[0][-1], input_encoding[1][-1]], axis=1)
            
        
        with tf.variable_scope("input_encoder_2"):
            # input encoder for finding the most relevant camera and attribute
            
            num_units = [self.embeddingSize, self.embeddingSize]
            #cells = [tf.nn.rnn_cell.LSTMCell(num_units=n, initializer=tf.initializers.glorot_normal()) for n in num_units]
            cells = [tf.nn.rnn_cell.GRUCell(num_units=n, kernel_initializer=tf.initializers.glorot_normal()) for n in num_units]
            
            stacked_rnn_cell = tf.contrib.rnn.MultiRNNCell(cells)
            
            _, input_encoding = tf.nn.dynamic_rnn(stacked_rnn_cell, self._input_one_hot, dtype=tf.float32)
            
            # for single layer GRU
            self._input_encoding_2 = input_encoding[-1] # TODO why is this here??? [-1] A: get output instead of candidate 
        
            # for two layer LSTM
            #self._input_encoding_2 = tf.concat([input_encoding[0][-1], input_encoding[1][-1]], axis=1)
            
        
        
        
        with tf.variable_scope("DB_matcher"):
            # find the best matching camera and attribute from the database
            
            cam1 = tf.layers.dense(self._input_encoding_2, self.numUniqueCams, activation=tf.nn.relu, kernel_initializer=tf.initializers.he_normal())
            #cam2 = tf.layers.dense(cam1, self.numUniqueCams, activation=tf.nn.relu, kernel_initializer=tf.initializers.he_normal())
            cam3 = tf.nn.softmax(cam1)
            
            att1 = tf.layers.dense(self._input_encoding_2, self.numUniqueAtts, activation=tf.nn.relu, kernel_initializer=tf.initializers.he_normal())
            #att2 = tf.layers.dense(att1, self.numUniqueAtts, activation=tf.nn.relu, kernel_initializer=tf.initializers.he_normal())
            att3 = tf.nn.softmax(att1)
            
            # use gumbel softmax to get a "one-hot" vector
            #self.camMatch = cam3 
            #self.attMatch = att3
            
            self.camMatch = tf.contrib.distributions.RelaxedOneHotCategorical(self.gumbel_softmax_temp, probs=cam3).sample()
            self.attMatch = tf.contrib.distributions.RelaxedOneHotCategorical(self.gumbel_softmax_temp, probs=att3).sample()
            
            self.camMatchIndex = tf.argmax(self.camMatch, axis=1)
            self.attMatchIndex = tf.argmax(self.attMatch, axis=1)
            
        
        with tf.variable_scope("DB_encoder"):
            # DB encoder
            self._db_entries = tf.placeholder(tf.float32, [self.batchSize, self.numUniqueCams, self.numUniqueAtts, self.dbSeqLen, self.vocabSize], name='DB_entries')
            
            
            # multiply by the match vectors and sum so that only the matching value remains
            self.db_match_val = tf.einsum("bcalv,ba->bclv", self._db_entries, self.attMatch)
            self.db_match_val = tf.einsum("bclv,bc->blv", self.db_match_val, self.camMatch)
            
            self.db_match_val_charindices = tf.argmax(self.db_match_val, axis=2)
            
            # bi-directional GRU
            num_units = [self.embeddingSize, self.embeddingSize]
            
            #cells_fw = [tf.nn.rnn_cell.LSTMCell(num_units=n, initializer=tf.initializers.glorot_normal()) for n in num_units]
            #cells_bw = [tf.nn.rnn_cell.LSTMCell(num_units=n, initializer=tf.initializers.glorot_normal()) for n in num_units]
            
            cells_fw = [tf.nn.rnn_cell.GRUCell(num_units=n, kernel_initializer=tf.initializers.glorot_normal()) for n in num_units]
            cells_bw = [tf.nn.rnn_cell.GRUCell(num_units=n, kernel_initializer=tf.initializers.glorot_normal()) for n in num_units]
            
            
            self.db_match_val_encoding, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw=cells_fw,
                                                                                            cells_bw=cells_bw,
                                                                                            inputs=self.db_match_val,
                                                                                            dtype=tf.float32)
        
        
        #
        # setup the decoder
        #
        self._ground_truth_outputs = tf.placeholder(tf.int32, [self.batchSize, self.outputSeqLen], name='true_robot_outputs')
        #self._ground_truth_outputs_one_hot = tf.one_hot(self._ground_truth_outputs, self.vocabSize)
        
        
        self.copynet_cell = copynet.CopyNetWrapper3(cell=tf.nn.rnn_cell.GRUCell(self.embeddingSize, kernel_initializer=tf.initializers.glorot_normal()),
                                                   encoder_states=self.db_match_val_encoding,
                                                   #encoder_input_ids=self._db_entries[:,0,0,:,:],
                                                   encoder_input_ids=self.db_match_val,
                                                   vocab_size=self.vocabSize)
        
        
        self.decoder_initial_state = self.copynet_cell.zero_state(self.batchSize, tf.float32).clone(cell_state=self._input_encoding)
        
        
        # append start char on to beginning of outputs so they can be used for teacher forcing - i.e. as inputs to the coipynet decoder
        after_slice = tf.strided_slice(self._ground_truth_outputs, [0, 0], [self.batchSize, -1], [1, 1]) # slice of the last char of each output sequence (is this necessary?)
        decoder_inputs = tf.concat( [tf.fill([self.batchSize, 1], charToIndex[goChar]), after_slice], 1) # concatenate on a go char onto the start of each output sequence
        
        self.decoder_inputs_one_hot = tf.one_hot(decoder_inputs, self.vocabSize)
        
        # rollout the decoder two times - once for use with teacher forcing (training) and once without (testing)
        # for training
        self._loss, self._train_predicted_output_sequences, self._train_copy_scores, self._train_gen_scores = self.build_decoder(teacherForcing=True, scopeName="decoder_train")
        
        #self._loss = tf.check_numerics(self._loss, "_loss")
        
        # for testing
        self._test_loss, self._test_predicted_output_sequences, self._test_copy_scores, self._test_gen_scores = self.build_decoder(teacherForcing=True, scopeName="decoder_test")
        
        
        #
        # setup the training function
        #
        opt = tf.train.AdamOptimizer(learning_rate=1e-4)
        
        gradients = opt.compute_gradients(self._loss)
        
        #tf.check_numerics(gradients, "gradients")
        
        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]
        
        self._train_op = opt.apply_gradients(capped_gradients, name="train_op")
        
        
        #
        # setup the prediction function
        #
        self._pred_op = tf.argmax(self._test_predicted_output_sequences, 2, name="predict_op")
        #self._pred_prob_op = tf.nn.softmax(predicted_output_sequences, axis=2, name="predict_prob_op")
        #self._pred_log_prob_op = tf.log(predict_proba_op, name="predict_log_proba_op")
        
        
        self._init_op = tf.initialize_all_variables()
        
        self.initialize()
        
    
    
    def build_decoder(self, teacherForcing=False, scopeName="decoder"):
        
        
        with tf.variable_scope(scopeName):
            
            loss = 0
            predicted_output_sequences = []
            copy_scores = []
            gen_scores = []
            
            state = self.decoder_initial_state
            output = self.decoder_inputs_one_hot[:, 0, :] # each output sequence must have a 'start' char appended to the beginning
            
            
            for i in range(self.outputSeqLen):
                
                #
                # TODO: don't use teacher forcing for prediction
                #
                
                if teacherForcing:
                    # if using teacher forcing
                    output, state, copy_score, gen_score = self.copynet_cell(self.decoder_inputs_one_hot[:, i, :], state)
                else:
                    # if not using teacher forcing
                    output, state, copy_score, gen_score = self.copynet_cell(output, state)
                
                
                predicted_output_sequences.append(tf.reshape(output, shape=(tf.shape(output)[0], 1, self.vocabSize)))
                copy_scores.append(tf.reshape(copy_score, shape=(tf.shape(copy_score)[0], 1, self.vocabSize)))
                gen_scores.append(tf.reshape(gen_score, shape=(tf.shape(gen_score)[0], 1, self.vocabSize)))
                
                # get the ground truth output
                ground_truth_output = tf.one_hot(self._ground_truth_outputs[:, i], self.vocabSize) # these are one-hot char encodings at timestep i
                
                # compute the loss
                #cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_output, logits=output)
                cross_entropy = tf.losses.softmax_cross_entropy(ground_truth_output, output, reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE) # is this right for sequence eval?
                #current_loss = tf.reduce_sum(cross_entropy)
                loss = loss + cross_entropy
            
            
            predicted_output_sequences = tf.concat(predicted_output_sequences, 1)
            copy_scores = tf.concat(copy_scores, 1)
            gen_scores = tf.concat(gen_scores, 1)
        
        return loss, predicted_output_sequences, copy_scores, gen_scores
    
    
    def initialize(self):
        self._sess = tf.Session()
        self._sess.run(self._init_op)
        
    
    def train(self, inputs, databases, groundTruthOutputs):
        feedDict = {self._inputs: inputs, self._db_entries: databases, self._ground_truth_outputs: groundTruthOutputs}
        
        trainingLoss, _ = self._sess.run([self._loss, self._train_op], feed_dict=feedDict)
        
        return trainingLoss
    
    
    def predict(self, inputs, databases, groundTruthOutputs):
        feedDict = {self._inputs: inputs, self._db_entries: databases, self._ground_truth_outputs: groundTruthOutputs}
        
        preds, copyScores, genScores, camMatchArgMax, attMatchArgMax, camMatch, attMatch = self._sess.run([self._pred_op, 
                                                                           self._test_copy_scores, 
                                                                           self._test_gen_scores,
                                                                           self.camMatchIndex,
                                                                           self.attMatchIndex,
                                                                           self.camMatch,
                                                                           self.attMatch], feed_dict=feedDict)
        
        return preds, copyScores, genScores, camMatchArgMax, attMatchArgMax, camMatch, attMatch



def normalized_edit_distance(s1, s2):
    return editdistance.eval(s1, s2) / float(max(len(s1), len(s2)))



def vectorize_sentences(sentences, charToIndex, maxSentLen):
    
    maxSentLen += 1 # for the EOS char
    
    sentVecs = []
    sentCharIndexLists = []
    
    for i in range(len(sentences)):
        
        sentVec = np.zeros(shape=(maxSentLen, len(charToIndex)))
        sentCharIndexList = []
        
        for j in range(maxSentLen):
            
            if j < len(sentences[i]):
                sentVec[j, charToIndex[sentences[i][j]]] = 1.0
                sentCharIndexList.append(charToIndex[sentences[i][j]])
            else:
                sentVec[j, charToIndex[" "]] = 1.0 # pad the end of sentences with spaces
                sentCharIndexList.append(charToIndex[" "])
        
        
        sentVec[-1, charToIndex[eosChar]] = 1
        sentCharIndexList.append(charToIndex[eosChar])
        
        sentVecs.append(sentVec)
        sentCharIndexLists.append(sentCharIndexList)
    
    
    return sentVecs, sentCharIndexLists



def vectorize_databases(databases, charToIndex, maxValLen):
    
    camToIndex = {}
    indexToCam = {}
    
    attToIndex = {}
    indexToAtt = {}
    
    
    # find the unique cameras and attributes
    for db in databases:
        
        for c in db.keys():
            if c not in camToIndex:
                camToIndex[c] = len(camToIndex)
                indexToCam[camToIndex[c]] = c
                
            for a in db[c]:            
                if a not in attToIndex:
                    attToIndex[a] = len(attToIndex)
                    indexToAtt[attToIndex[a]] = a
    
    
    # encode each db
    dbEncodings = []
    
    for db in databases:
        dbEnc = []
        
        for i in range(len(indexToCam)):
            c = indexToCam[i]
            row = []
            
            for j in range(len(indexToAtt)):
                a = indexToAtt[j]
                
                valEnc, valIndexList = vectorize_sentences([db[c][a]], charToIndex, maxValLen)
                valEnc = valEnc[0]
                valIndexList = valIndexList[0]
                row.append(valEnc)
            
            dbEnc.append(row)
        
        dbEncodings.append(dbEnc)    
    
    return dbEncodings, camToIndex, indexToCam, attToIndex, indexToAtt



def unvectorize_sentences(sentCharIndexLists, indexToChar):
    
    sentences = []
    
    for i in range(sentCharIndexLists.shape[0]):
        
        sent = ""
        
        for j in range(sentCharIndexLists.shape[1]):
            
            sent += indexToChar[sentCharIndexLists[i,j]]
        
        sentences.append(sent)
        
    return sentences



def color_results(outputStringsPred, outputIndexGt, copyScores, genScores, charToIndex):
    
    coloredOutputStrings = []
    
    copyScoresPred = []
    genScoresPred = []
    
    copyScoresGt = []
    genScoresGt = []
    
    
    for i in range(len(outputStringsPred)):
        colOutStr = ""
        
        cScoresPred = []
        gScoresPred = []
        
        cScoresGt = []
        gScoresGt = []
        
        
        for j in range(len(outputStringsPred[i])):
            cs = copyScores[i, j, charToIndex[outputStringsPred[i][j]]]
            gs = genScores[i, j, charToIndex[outputStringsPred[i][j]]]
            
            csGt = copyScores[i, j, outputIndexGt[i][j]]
            gsGt = genScores[i, j, outputIndexGt[i][j]]
            
            
            # color the char if it was coppied from the input
            if cs > gs:
                colOutStr += "\x1b[36m" + outputStringsPred[i][j] + "\x1b[0m" # blue-green
                
            elif cs == gs:
                colOutStr += "\x1b[33m" + outputStringsPred[i][j] + "\x1b[0m" # yellow
            
            else:
                colOutStr += outputStringsPred[i][j]
            
            
            cScoresPred.append(cs)
            gScoresPred.append(gs)
            
            cScoresGt.append(csGt)
            gScoresGt.append(gsGt)
            
        
        coloredOutputStrings.append(colOutStr)
        
        
        copyScoresPred.append(cScoresPred)
        genScoresPred.append(gScoresPred)
        
        copyScoresGt.append(cScoresGt)
        genScoresGt.append(gScoresGt)
        
        
    return coloredOutputStrings, copyScoresPred, genScoresPred, copyScoresGt, genScoresGt



if __name__ == "__main__":
    
    #
    # simulate some interactions
    #
    databases = []
    inputs = []
    outputs = []
    
    
    database = {}
    database["Canon"] = {}
    database["Canon"]["LOCATION"] = "DISPLAY_1"
    database["Canon"]["PRICE"] = "$99"
    database["Nikon"] = {}
    database["Nikon"]["LOCATION"] = "DISPLAY_2"
    database["Nikon"]["PRICE"] = "$88"
    
    
    # example 1
    databases.append(database)
    inputs.append("C | DISPLAY_1 | How much is this one?")
    outputs.append("S | DISPLAY_1 | This one is $99.")
    
    
    # example 2
    databases.append(database)
    inputs.append("C | DISPLAY_2 | How much is this one?")
    outputs.append("S | DISPLAY_2 | This one is $88.")
    
    
    
    database = {}
    database["Canon"] = {}
    database["Canon"]["LOCATION"] = "DISPLAY_1"
    database["Canon"]["PRICE"] = "$77"
    database["Nikon"] = {}
    database["Nikon"]["LOCATION"] = "DISPLAY_2"
    database["Nikon"]["PRICE"] = "$66"
    
    
    # example 3
    databases.append(database)
    inputs.append("C | DISPLAY_1 | How much is this one?")
    outputs.append("S | DISPLAY_1 | This one is $77.")
    
    
    # example 4
    databases.append(database)
    inputs.append("C | DISPLAY_2 | How much is this one?")
    outputs.append("S | DISPLAY_2 | This one is $66.")
    
    
    #
    #
    #
    database = {}
    database["Canon"] = {}
    database["Canon"]["LOCATION"] = "DISPLAY_1"
    database["Canon"]["PRICE"] = "$88"
    database["Nikon"] = {}
    database["Nikon"]["LOCATION"] = "DISPLAY_2"
    database["Nikon"]["PRICE"] = "$99"
    
    # example 5
    databases.append(database)
    inputs.append("C | DISPLAY_1 | How much is this one?")
    outputs.append("S | DISPLAY_1 | This one is $88.")
    
    
    # example 6
    databases.append(database)
    inputs.append("C | DISPLAY_2 | How much is this one?")
    outputs.append("S | DISPLAY_2 | This one is $99.")
    
    
    database = {}
    database["Canon"] = {}
    database["Canon"]["LOCATION"] = "DISPLAY_1"
    database["Canon"]["PRICE"] = "$66"
    database["Nikon"] = {}
    database["Nikon"]["LOCATION"] = "DISPLAY_2"
    database["Nikon"]["PRICE"] = "$77"
    
    
    # example 5
    databases.append(database)
    inputs.append("C | DISPLAY_1 | How much is this one?")
    outputs.append("S | DISPLAY_1 | This one is $66.")
    
    
    # example 6
    databases.append(database)
    inputs.append("C | DISPLAY_2 | How much is this one?")
    outputs.append("S | DISPLAY_2 | This one is $77.")
    
    
    #
    # calculate the data sizes
    #
    numExamples = len(outputs)
    
    inputSentLens = [len(i) for i in inputs]
    
    outputSentLens = [len(o) for o in outputs]
    
    dbValLens = []
    dbVals = []
    
    for db in databases:
        for c in db.keys():
            for a in db[c].keys():
                dbValLens.append(len(db[c][a]))
                dbVals.append(db[c][a])
    
    
    maxInputSentLen = max(inputSentLens)
    maxOutputSentLen = max(outputSentLens)
    maxDbValLen = max(dbValLens)
    
    
    
    #
    # create the char vocab
    #
    uniqueChars = [eosChar, goChar]
    
    for v in dbVals:
        for c in v:
            if c not in uniqueChars:
                uniqueChars.append(c)
    
    for i in inputs:
        for c in i:
            if c not in uniqueChars:
                uniqueChars.append(c)
    
    for o in outputs:
        for c in o:
            if c not in uniqueChars:
                uniqueChars.append(c)
    
    
    #
    # char to index encoder
    # make sure all chars and nums are included so that the network can generalize
    #
    alphanum = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
    
    for i in alphanum:
        if i not in uniqueChars:
            uniqueChars.append(i)
    
    charToIndex = {}
    indexToChar = {}
    
    for i in range(len(uniqueChars)):
        charToIndex[uniqueChars[i]] = i
        indexToChar[i] = uniqueChars[i]
    
    numUniqueChars = len(uniqueChars)
    
    
    #
    # encode the data
    #
    inputOneHotVectors, inputIndexLists = vectorize_sentences(inputs, charToIndex, max(inputSentLens))
    outputOneHotVectors, outputIndexLists = vectorize_sentences(outputs, charToIndex, max(outputSentLens))
    
    dbEncodings, camToIndex, indexToCam, attToIndex, indexToAtt = vectorize_databases(databases, charToIndex, max(dbValLens))
   
    
    
    #
    # setup the model
    #
    batchSize = 4
    embeddingSize = 30
    
    inputs = inputIndexLists
    groundTruthOutputs = outputIndexLists
    
    # add one to account for the eos char
    maxInputSeqLen = maxInputSentLen + 2
    maxOutputSeqLen = maxOutputSentLen + 2
    maxDbSeqLen = maxDbValLen + 1
    
    
    #
    # split training and testing data
    #
    trainInputs = inputs[:4]
    testInputs = inputs[4:]
    
    trainDbs = dbEncodings[:4]
    testDbs = dbEncodings[4:]
    
    trainGroundTruth = groundTruthOutputs[:4]
    testGroundTruth = groundTruthOutputs[4:]
    
    
    # for computing accuracy
    trainGroundTruthFlat = []
    testGroundTruthFlat = []
    
    for i in range(numExamples):    
        groundTruthFlat = groundTruthOutputs[i]
        
        if i < 4:
            trainGroundTruthFlat += groundTruthFlat
        else:
            testGroundTruthFlat += groundTruthFlat
    
    
    #
    # setup the network
    #
    learner = CustomNeuralNetwork(inputSeqLen=maxInputSeqLen, 
                                  dbSeqLen=maxDbSeqLen, 
                                  outputSeqLen=maxOutputSeqLen,
                                  numUniqueCams=len(camToIndex),
                                  numUniqueAtts=len(attToIndex),
                                  batchSize=batchSize, 
                                  vocabSize=numUniqueChars,
                                  embeddingSize=embeddingSize,
                                  charToIndex=charToIndex)
    
    
    numEpochs = 20000
    
    for e in range(numEpochs):
        
        trainCost = learner.train(trainInputs, trainDbs, trainGroundTruth)
        #print trainCost
        
        if e % 100 == 0:
            
            #
            # compute accuracy, etc.
            #
            
            # TRAIN
            trainPreds, trainCopyScores, trainGenScores, trainCamMatchArgMax, trainAttMatchArgMax, trainCamMatch, trainAttMatch = learner.predict(trainInputs, trainDbs, trainGroundTruth)
            
            
            trainAcc = 0.0
            for i in range(len(trainPreds)):
                trainAcc += normalized_edit_distance(trainGroundTruth[i], trainPreds[i])
            trainAcc /= len(trainGroundTruth)
            
            #trainPredsFlat = np.array(trainPreds).flatten()
            #trainAcc = metrics.accuracy_score(trainPredsFlat, trainGroundTruthFlat)
            trainPredSents = unvectorize_sentences(trainPreds, indexToChar)
            trainPredSents2, trainCopyScores2, trainGenScores2, trainCopyScoresGt, trainGenScoresGt = color_results(trainPredSents, 
                                                                                                                    trainGroundTruth,
                                                                                                                    trainCopyScores, 
                                                                                                                    trainGenScores, 
                                                                                                                    charToIndex)
            
            
            # TEST
            testPreds, testCopyScores, testGenScores, testCamMatchArgMax, testAttMatchArgMax, testCamMatch, testAttMatch = learner.predict(testInputs, testDbs, testGroundTruth)
            
            testAcc = 0.0
            for i in range(len(testPreds)):
                testAcc += normalized_edit_distance(testGroundTruth[i], testPreds[i])
            testAcc /= len(testGroundTruth)
            
            #testPredsFlat = np.array(testPreds).flatten()
            #testAcc = metrics.accuracy_score(testPredsFlat, testGroundTruthFlat)
            testPredSents = unvectorize_sentences(testPreds, indexToChar)
            testPredSents2, testCopyScores2, testGenScores2, testCopyScoresGt, testGenScoresGt = color_results(testPredSents,
                                                                                                               testGroundTruth, 
                                                                                                               testCopyScores, 
                                                                                                               testGenScores, 
                                                                                                               charToIndex)
            
            
            print "****************************************************************"
            print "TRAIN", e, round(trainCost, 3), round(trainAcc, 3)
            
            for i in range(len(trainPredSents2)):
                print "TRUE:", outputs[i]
                print "PRED:", trainPredSents2[i]
                print "\x1b[31m"+ indexToCam[trainCamMatchArgMax[i]] +" "+ indexToAtt[trainAttMatchArgMax[i]] +" "+ databases[i][indexToCam[trainCamMatchArgMax[i]]][indexToAtt[trainAttMatchArgMax[i]]] +"\x1b[0m", trainCamMatch[i], trainAttMatch[i]
                print 
            
            print
            
            print "TEST", e, round(testAcc, 3)
            
            for i in range(len(testPredSents2)):
                print "TRUE:", outputs[i+4]
                print "PRED:", testPredSents2[i]
                print "\x1b[31m"+ indexToCam[testCamMatchArgMax[i]] +" "+ indexToAtt[testAttMatchArgMax[i]] +" "+ databases[i+4][indexToCam[testCamMatchArgMax[i]]][indexToAtt[testAttMatchArgMax[i]]] +"\x1b[0m", testCamMatch[i], testAttMatch[i]
                print
        
            print "****************************************************************"
            
            
            
            with open(sessionDir+"/{:}_outputs.csv".format(e), "wb") as csvfile:
                
                writer = csv.writer(csvfile)
                
                writer.writerow(["TRAIN", round(trainCost, 3), round(trainAcc, 3)])
                
                for i in range(len(trainPredSents)):
                    writer.writerow(["TRUE:"] + [c for c in outputs[i]])
                    writer.writerow(["PRED:"] + [c for c in trainPredSents[i]])
                    
                    writer.writerow(["PRED COPY:"] + [c for c in trainCopyScores2[i]])
                    writer.writerow(["PRED GEN:"] + [c for c in trainGenScores2[i]])
                    
                    writer.writerow(["TRUE COPY:"] + [c for c in trainCopyScoresGt[i]])
                    writer.writerow(["TRUE GEN:"] + [c for c in trainGenScoresGt[i]])
                    
                    writer.writerow([])
                
                
                writer.writerow(["TEST", round(testAcc, 3)])
                
                for i in range(len(testPredSents)):
                    writer.writerow(["TRUE:"] + [c for c in outputs[i+4]])
                    writer.writerow(["PRED:"] + [c for c in testPredSents[i]])
                    
                    writer.writerow(["PRED COPY:"] + [c for c in testCopyScores2[i]])
                    writer.writerow(["PRED GEN:"] + [c for c in testGenScores2[i]])
                    
                    writer.writerow(["TRUE COPY:"] + [c for c in testCopyScoresGt[i]])
                    writer.writerow(["TRUE GEN:"] + [c for c in testGenScoresGt[i]])    
                    
                    writer.writerow([])
            
            
            if trainAcc == 0.0:
                print "training error is 0.0"
                break








